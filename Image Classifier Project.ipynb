{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flowers'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data using torch vision\n",
    "data_transforms = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                             ])\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                      transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "# Loading datasets with ImageFolder\n",
    "data_data_sets = datasets.ImageFolder(data_dir, transform=data_transforms)\n",
    "train_data_set = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_data_set = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "test_data_set = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "#Using the image datasets and the trainforms\n",
    "data_loaders = torch.utils.data.DataLoader(data_data_sets, batch_size = 32, shuffle = True)\n",
    "train_loaders = torch.utils.data.DataLoader(train_data_set, batch_size = 32, shuffle = True)\n",
    "valid_loaders = torch.utils.data.DataLoader(valid_data_set, batch_size = 32, shuffle = True)\n",
    "test_loaders = torch.utils.data.DataLoader(test_data_set, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label mapping using JSON\n",
    "#will give you a dictionary mapping the integer encoded categories to the actual names of the flowers.\n",
    "import json\n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Network\n",
    "model = models.vgg19(pretrained=True)\n",
    "print(model)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    \n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                                        ('fc1', nn.Linear(25088, 4096)),\n",
    "                                        ('relu', nn.ReLU()),\n",
    "                                        ('fc2', nn.Linear(4096, 102)),\n",
    "                                        ('fc3', nn.LogSoftmax(dim=1))\n",
    "                                        ]))\n",
    "\n",
    "model.classifier = classifier\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Network\n",
    "def train_model(model, trainloader, validloader, epochs, print_every, criterion, optimizer, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    epochs = epochs\n",
    "    print_every = print_every\n",
    "    steps = 0\n",
    "\n",
    "    # change to cuda\n",
    "    model.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for ii, (inputs, labels) in enumerate(trainloader):\n",
    "            steps += 1\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward passes\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            #Calculating Loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                valid_accuracy = check_validation_set(validloader,device)\n",
    "                print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                      \"Validation Accuracy: {}\".format(round(valid_accuracy,4)))\n",
    "\n",
    "                running_loss = 0\n",
    "    \n",
    "    \n",
    "def check_validation_set(valid_loader,device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total \n",
    "\n",
    "\n",
    "\n",
    "train_model(model, train_loaders, valid_loaders, 3, 40, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Network\n",
    "def check_accuracy_on_test(test_loaders,device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loaders:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    return correct / total \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_on_test(test_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving checkpoint\n",
    "model.class_to_idx = train_data_set.class_to_idx\n",
    "\n",
    "checkpoint = {'transfer_model': 'vgg19',\n",
    "              'input_size': 25088,\n",
    "              'output_size': 102,\n",
    "              'features': model.features,\n",
    "              'classifier': model.classifier,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'idx_to_class': {v: k for k, v in train_data_set.class_to_idx.items()}\n",
    "             }\n",
    "\n",
    "torch.save(checkpoint, 'check.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading up the Checkpoint\n",
    "def load_checkpoint(path):\n",
    "    model_information = torch.load(path)\n",
    "    model = models.vgg19(pretrained=True)\n",
    "    classifier = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(25088, 4096)),\n",
    "                              ('relu', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(4096, 102)),\n",
    "                              ('output', nn.LogSoftmax(dim=1))\n",
    "                              ]))\n",
    "\n",
    "    model.classifier = classifier\n",
    "    model.load_state_dict(model_information['state_dict'])\n",
    "    \n",
    "    return model, model_information\n",
    "\n",
    "model, model_information = load_checkpoint('check.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "\n",
    "    \n",
    "    #  Processing a PIL image for use in a PyTorch model\n",
    "    adj= transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return adj(Image.open(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method Used to CHECK by converting pytorch tensors and displayin them\n",
    "\n",
    "def imshow(image, ax=None, title=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "   \n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    \n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "#Using \"topk\"to display the top 5 most probable classes\n",
    "def predict(image_path, model, topk=5):\n",
    "    \n",
    "    model.to('cuda')\n",
    "    img_torch = process_image(image_path)\n",
    "    img_torch = img_torch.unsqueeze_(0).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.forward(img_torch.cuda())\n",
    "        \n",
    "    probability = F.softmax(output.data,dim=1)\n",
    "    \n",
    "    return probability.topk(topk)\n",
    "\n",
    "img = (\"flowers/test/10/image_07104.jpg\")\n",
    "predict(img, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
